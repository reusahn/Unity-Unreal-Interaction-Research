# 🧭 Unity–Unreal Interfacing Research  
by **Jonghoon Ahn**  

A collection of experimental projects developed in real-time game engine environments using Unity and Unreal Engine.  
The works explore how artistic visual styles can be recreated and implemented within real-time systems, extending into XR research through VR and AR technologies.  
They also include the creation of digital humans through 3D scanning, Gaussian splatting, and generative image or video synthesis.  

Each project combines artistic inquiry with technical experimentation to expand motion, emotion, and data into new forms of interactive expression.

---

## 🧰 Tools & Technologies  
**Engines:** Unity, Unreal Engine 5  
**Languages:** C#, C++, Python  
**SDKs:** OpenXR, AR Foundation, Azure Kinect SDK, OpenCV, Barracuda  
**Hardware:** Azure Kinect, Meta Quest 3, OptiTrack, Leap Motion  
**Other Tools:** Maya, Blender, VFX Graph, Git  

---

## 🧑‍💻 Research Focus  
This collection bridges art and engineering, using game engines as platforms for experimentation in human–machine interfacing, immersive visualization, and digital empathy.  
It demonstrates how real-time tools can serve both creative and scientific inquiry.

---

## 👁️‍🗨️ 01. Digital Human & Virtual Beings  
Explores empathy, embodiment, and the perception of artificial life through motion capture, MetaHuman, and AI-driven characters.  
**Keywords:** Unreal Engine · MetaHuman · Motion Capture · AI Simulation  

This series traces the evolution of empathy toward digital beings.  
It begins with *Gang Sehwang*, a digital reconstruction of a historical painter, and evolves through *AI Choi Jung Hoon*, where an AI performs a real person’s identity using LLMs.  
From *AI ZOO* to *Seon A’s Family* and *Whispers*, the works explore confinement, reconciliation, and emotional reconstruction—culminating in collaborative empathy with artists like Yaloo and Scott.  

**Projects:**  
- **Gang Sehwang** — Classical Korean painter reimagined as a digital human; reinterpretation of Joseon-era aesthetics through 3D embodiment.  
- **Choi JungHoon** — Data-based self-simulation exploring fragmented identity and machine perception.  
- **AI ZOO** — Interactive installation about AI confinement and empathy toward synthetic beings.  
- **Seon A’s Family** — Virtual family portrait bridging memory, ancestry, and digital embodiment.  
- **Whispers** — Poetic digital short exploring silence, loss, and emotional reconstruction.  
- **Yaloo Collaboration** — Shared virtual performance merging live motion and digital avatars.  
- **Shin Inho** — MetaHuman study exploring digital realism and body simulation.  
- **Scott Collaboration** — Multi-avatar experiment with generative gestures and collective behavior.  

---

## 🪞 02. AR-based Style Transfer  
Merges physical and digital aesthetics using AR, real-time rendering, and neural style transfer techniques.  
**Keywords:** Unity AR Foundation · Neural Style Transfer · Real-time Texture Mapping  

This series explores how visual identity transforms when physical reality and digital aesthetics overlap.  
Through real-time rendering and neural style transfer in Unity, these works recreate the brushstrokes and visual logic of other artists—such as **Picasso’s cubism, Joseon-era comics, and Erin’s hand-drawn style**—to generate entirely new digital forms.  
Projects like *Picasso*, *Jemulpo Photo Studio*, and *Beads Wall* turn gestures, camera feeds, and motion data into living canvases, where historical and contemporary sensibilities merge in real time.  

**Projects:**  
- **Picasso** — AR experiment applying classical painting styles to live camera feeds.  
- **Jemulpo Photo Studio** — AR portrait experience inspired by historical photography and cultural hybridity.  
- **Erin** — AR installation visualizing emotion through stylized filters and gesture-based feedback.  
- **Beads Wall** — Large-scale interactive wall using Azure Kinect motion data to drive particle-based visualization.  

---

## 🧠 03. Sensor-based Interaction (Azure Kinect)  
Investigates how sensor-based perception bridges the physical and digital space through motion data, body tracking, and spatial choreography.  
**Keywords:** Azure Kinect · OpenCV · Depth Sensing · Motion Visualization  

This group investigates how body data becomes a language between humans and machines.  
In works like *Silhak Dance* and *To Eternity*, physical gestures are captured and transformed into visual compositions, creating choreographies of memory, resistance, and time.  

**Projects:**  
- **Silhak Dance** — Interactive performance using body movement as data to generate visual compositions.  
- **To Eternity** — Depth-based choreography exploring continuity between motion, space, and time.  

---

## 📎 Contact  
**Portfolio:** [jonghoonahn.com](https://jonghoonahn.com)  
**GitHub:** [github.com/reusahn/Unity-Unreal-Interaction-Research](https://github.com/reusahn/Unity-Unreal-Interaction-Research/tree/main)  
**Email:** [reusahn@gmail.com](mailto:reusahn@gmail.com)  
**Resume:** Resume_2025_JonghoonAhn.pdf  

---

© 2025 Jonghoon Ahn. All rights reserved.
